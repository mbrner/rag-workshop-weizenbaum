{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2be816-1286-4b13-9978-5c599d2ca867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: \"Hello, I'm a language model, and my project will get better with time, but I think there are a lot more things that can help you\"\n",
      "Generation 2: \"Hello, I'm a language model, not a language model, so if I don't have a problem, I can fix it by creating new words\"\n",
      "Generation 3: \"Hello, I'm a language model, and I'm trying to learn some stuff. I'll try to do some basic programming and just learn better ways\"\n",
      "Generation 4: \"Hello, I'm a language model, but I don't believe in grammar. This will work for every language model. You can define it very quickly\"\n",
      "Generation 5: \"Hello, I'm a language model, a model of how things should be, and then we look at different things as well.\" I'd like to\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "gens = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "for i, g in enumerate(gens):\n",
    "    print(f'Generation {i+1}: \"{g[\"generated_text\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2120a7fb-03b9-4e7c-8b96-d2885fb70ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Poor English input: I'd be more than happy to work with you in another project.\n",
      "Good English output: The requested changes have been made. or I might become involved in your project. or I might become associated with you in your next project.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n",
      "Poor English input: She don't like apples.\n",
      "Good English output: I'll just leave out her. she like apples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3:\n",
      "Poor English input: He do his homework everyday.\n",
      "Good English output: He do his homework everyday.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4:\n",
      "Poor English input: Me and him went to the store.\n",
      "Good English output: That means he and me went to the store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5:\n",
      "Poor English input: I have saw that movie last week.\n",
      "Good English output: I see that movie last week.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 6:\n",
      "Poor English input: They was going to the party.\n",
      "Good English output: Thank you for having a great time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7:\n",
      "Poor English input: I ain't got no money.\n",
      "Good English output: I'm lazy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 8:\n",
      "Poor English input: Her like cooking and dancing.\n",
      "Good English output: I enjoyed that she lived.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 9:\n",
      "Poor English input: We was excited about the trip.\n",
      "Good English output: The mentioned changes have done. or I brought the new characters. or I brought the new characters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10:\n",
      "Poor English input: This is the most quickest route.\n",
      "Good English output: I wish I knew the first place you chose to pick your designer. If you pick to be picked next, the pick to be picked is the most quickest way ever to pick a designer. If they selected you, then pick the first one. or I selected you. or I did the alteration that you requested. or I altered things you wanted and did the modifications.\n",
      "Example 11:\n",
      "Poor English input: I can't hardly wait for the event.\n",
      "Good English output: It's a close fight in the end. or that I didn't make it. or that I will not win. or that I just don't feel anything at all. or that I didn't make any changes.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Poor English input: I eated the purple berries.\n",
    "Good English output: I ate the purple berries.\n",
    "\n",
    "Poor English input: Thank you for picking me as your designer. I’d appreciate it.\n",
    "Good English output: Thank you for choosing me as your designer. I appreciate it.\n",
    "\n",
    "Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications.\n",
    "Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications.\n",
    "\n",
    "Poor English input: {input}\n",
    "Good English output:\"\"\"\n",
    "\n",
    "examples = [\n",
    "    (\"I'd be more than happy to work with you in another project.\", \"I'd be more than happy to work with you on another project.\"),\n",
    "    (\"She don't like apples.\", \"She doesn't like apples.\"),\n",
    "    (\"He do his homework everyday.\", \"He does his homework every day.\"),\n",
    "    (\"Me and him went to the store.\", \"He and I went to the store.\"),\n",
    "    (\"I have saw that movie last week.\", \"I saw that movie last week.\"),\n",
    "    (\"They was going to the party.\", \"They were going to the party.\"),\n",
    "    (\"I ain't got no money.\", \"I don't have any money.\"),\n",
    "    (\"Her like cooking and dancing.\", \"She likes cooking and dancing.\"),\n",
    "    (\"We was excited about the trip.\", \"We were excited about the trip.\"),\n",
    "    (\"This is the most quickest route.\", \"This is the quickest route.\"),\n",
    "    (\"I can't hardly wait for the event.\", \"I can hardly wait for the event.\")\n",
    "]\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "gens = generator(prompt, max_length=256, num_return_sequences=n_completions, )\n",
    "\n",
    "for i, (in_, out_) in enumerate(examples):\n",
    "    gen = generator(prompt.format(input=in_), max_length=256, num_return_sequences=1, )[0]\n",
    "    lines = gen[\"generated_text\"].split(\"\\n\")\n",
    "    print(f'Example {i+1}:')\n",
    "    print('\\n'.join([lines[i] for i in range(len(lines)) if in_ in lines[i] or in_ in lines[i-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a42e0ec-7dab-4ee6-a2f5-8c7e2e805c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poor English input: I eated the purple berries.\\nGood English output: I ate the purple berries.\\n\\nPoor English input: Thank you for picking me as your designer. I’d appreciate it.\\nGood English output: Thank you for choosing me as your designer. I appreciate it.\\n\\nPoor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications.\\nGood English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications.\\n\\nPoor English input: I’d be more than happy to work with you in another project. Thank you very much. or I’d be more than happy to work with you in another project. Thank you very much.\\n\\nPoor English input: Thank you very much. or I ’d be more than happy to work with the designer. I’d be more than happy to work with you in another project. Thank you very much.\\n\\nPoor English input: Thanks for doing this. Thanks very much.\\n\\nPoor English input: Thanks for providing the suggestions. Thanks very much.\\n\\nIf'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d5cff59-44e7-4ab0-a3c0-412398570799",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Poor English input: I eated the purple berries.\\nGood English output: I ate the purple berries.\\n\\nPoor English input: Thank you for picking me as your designer. I’d appreciate it.\\nGood English output: Thank you for choosing me as your designer. I appreciate it.\\n\\nPoor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications.\\nGood English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications.\\n\\nPoor English input: I’d be more than happy to work with you in another project. Thank you very much. or I’d be more than happy to work with you in another project. Thank you very much.\\n\\nPoor English input: Thank you very much. or I ’d be more than happy to work with the designer. I’d be more than happy to work with you in another project. Thank you very much.\\n\\nPoor English input: Thanks for doing this. Thanks very much.\\n\\nPoor English input: Thanks for providing the suggestions. Thanks very much.\\n\\nIf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "af2a536e-8d51-4c0c-a07e-3fcaeac41de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Thank you very much.\\n\\nPoor English input: Thanks for doing this. Thanks very much.\\n\\nPoor English input: Thanks for providing the suggestions. Thanks very much.\\n\\nIf'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split(\"I’d be more than happy to work with you in another project.\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a7b42cf-cd47-45da-a603-169795ffedd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I absolutely love this product! It exceeded my expectations.\", \"Positive\"),\n",
    "    (\"This is the worst purchase I've ever made. Totally disappointed.\", \"Negative\"),\n",
    "    (\"I am extremely satisfied with my purchase. Highly recommend!\", \"Positive\"),\n",
    "    (\"The product quality is poor and the service was terrible.\", \"Negative\"),\n",
    "    (\"Amazing quality and fantastic customer service.\", \"Positive\"),\n",
    "    (\"Not worth the money at all.\", \"Negative\"),\n",
    "    (\"Absolutely perfect! I will buy again.\", \"Positive\"),\n",
    "    (\"It broke after just one use. Very unhappy.\", \"Negative\"),\n",
    "    (\"Exceptional performance and great value for money.\", \"Positive\"),\n",
    "    (\"I regret buying this. It didn’t meet my needs.\", \"Negative\"),\n",
    "    (\"Best investment I've ever made!\", \"Positive\"),\n",
    "    (\"Terrible, it did not work as described.\", \"Negative\"),\n",
    "    (\"Exceeded my expectations in every way.\", \"Positive\"),\n",
    "    (\"The product is subpar and I am not satisfied.\", \"Negative\"),\n",
    "    (\"I’m thrilled with the quality and the ease of use.\", \"Positive\"),\n",
    "    (\"Disappointing and frustrating experience.\", \"Negative\"),\n",
    "    (\"Five stars for an outstanding product.\", \"Positive\"),\n",
    "    (\"Do not waste your money on this.\", \"Negative\"),\n",
    "    (\"Incredible quality and design.\", \"Positive\"),\n",
    "    (\"Very unsatisfied, will not purchase again.\", \"Negative\"),\n",
    "    (\"Impressive performance and excellent build.\", \"Positive\"),\n",
    "    (\"The product failed to deliver any of its promises.\", \"Negative\"),\n",
    "    (\"I am in love with this item; it’s amazing!\", \"Positive\"),\n",
    "    (\"Not as advertised, completely let down.\", \"Negative\"),\n",
    "    (\"Highly recommend to anyone looking for quality.\", \"Positive\"),\n",
    "    (\"The worst product I've encountered.\", \"Negative\"),\n",
    "    (\"Superb and delightful, a true gem.\", \"Positive\"),\n",
    "    (\"Extremely disappointed with the performance.\", \"Negative\"),\n",
    "    (\"A brilliant buy, worth every penny.\", \"Positive\"),\n",
    "    (\"I wish I hadn't bought this, it's awful.\", \"Negative\"),\n",
    "    (\"Absolutely delighted with my experience.\", \"Positive\"),\n",
    "    (\"The quality is below expectations.\", \"Negative\"),\n",
    "    (\"Marvelous product with top-notch features.\", \"Positive\"),\n",
    "    (\"Unreliable and a waste of money.\", \"Negative\"),\n",
    "    (\"Truly a remarkable and enjoyable product.\", \"Positive\"),\n",
    "    (\"Regrettably, it does not function properly.\", \"Negative\"),\n",
    "    (\"The best purchase I made this year.\", \"Positive\"),\n",
    "    (\"Extremely poor design and performance.\", \"Negative\"),\n",
    "    (\"I couldn’t be happier with this amazing product.\", \"Positive\"),\n",
    "    (\"It is completely unsatisfactory and disappointing.\", \"Negative\"),\n",
    "    (\"An excellent product that I use daily.\", \"Positive\"),\n",
    "    (\"Not worth the hassle and money.\", \"Negative\"),\n",
    "    (\"Quality beyond expectations, very happy with it.\", \"Positive\"),\n",
    "    (\"A complete letdown, do not buy this.\", \"Negative\"),\n",
    "    (\"Super quality and amazing service.\", \"Positive\"),\n",
    "    (\"The product is defective and poorly made.\", \"Negative\"),\n",
    "    (\"Absolutely brilliant, it does everything promised.\", \"Positive\"),\n",
    "    (\"Completely ineffective and a total waste.\", \"Negative\"),\n",
    "    (\"A joy to use and very reliable.\", \"Positive\"),\n",
    "    (\"I regret this purchase immensely.\", \"Negative\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"\"\"Classifiy the sentiment with either Positive/Negative (no explnation only Positive/Negative as an answer):\n",
    "\n",
    "Review: \"I absolutely love this product! It exceeded my expectations.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"This is the worst purchase I've ever made. Totally disappointed.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "\n",
    "Review: \"This is the best purchase I've ever made. Totally amazed!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"{review}\"\n",
    "Sentiment:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd2fc425-60c7-47bb-a8a4-1a9366c7eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"\"\"Review: \"I absolutely love this product! It exceeded my expectations.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"This is the worst purchase I've ever made. Totally disappointed.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "\n",
    "Review: \"This is the best purchase I've ever made. Totally amazed!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"The quality is great and it works as advertised.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf1688db-b4d0-4a34-9961-e0704ec5719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<--- 0 --->\n",
      "<--- 0 --->\n",
      "<--- 1 --->\n",
      "<--- 1 --->\n",
      "The sentiment of the reviews is predominantly positive or negative, depending on the review. However, some reviews show a mix of both, as they contain elements of both positive and negative statements. The review that received the most positive sentiment is about a product being the best purchase ever made, while the negative one is about a product being the worst purchase ever made. Overall, the sentiment of the reviews leans towards positivity.\n",
      "---\n",
      "The sentiment of the reviews is predominantly positive or negative, depending on the review. However, some reviews show a mix of both, as they contain elements of both positive and negative statements. The review that received the most positive sentiment is about a product being the best purchase ever made, while the negative one is about a product being the worst purchase ever made. Overall, the sentiment of the reviews leans towards positivity.\n",
      "<--- 1 --->\n",
      "<--- 2 --->\n",
      "<--- 2 --->\n",
      "<--- 3 --->\n",
      "<--- 3 --->\n",
      "1. To classify the sentiment in the reviews, you can create a dictionary with the labels \"Positive\" and \"Negative\" as keys, and use a loop to iterate through the reviews and check if the sentiment is positive or negative. If it's positive, add it to the Positive label and vice versa for the Negative label.\n",
      "---\n",
      "\n",
      "1. To classify the sentiment in the reviews, you can create a dictionary with the labels \"Positive\" and \"Negative\" as keys, and use a loop to iterate through the reviews and check if the sentiment is positive or negative. If it's positive, add it to the Positive label and vice versa for the Negative label.\n",
      "\n",
      "2. To classify the sentiment, you can use natural language processing techniques like sentiment analysis to determine the overall sentiment of the reviews. You can also manually classify each review as positive or negative by looking at the words used and the context of the review.\n",
      "<--- 3 --->\n",
      "<--- 4 --->\n",
      "<--- 4 --->\n",
      "<--- 5 --->\n",
      "<--- 5 --->\n",
      "The sentiment of the review would be \"Positive\" for the first review, \"Negative\" for the second review, and \"Positive\" for the third review.\n",
      "---\n",
      "\n",
      "The sentiment of the review would be \"Positive\" for the first review, \"Negative\" for the second review, and \"Positive\" for the third review.\n",
      "<--- 5 --->\n",
      "<--- 6 --->\n",
      "<--- 6 --->\n",
      "<--- 7 --->\n",
      "<--- 7 --->\n",
      "Yes, the sentiment for the reviews can be classified as either positive or negative based on the words \"loved\" and \"disappointed\" in the review.\n",
      "---\n",
      "\n",
      "Yes, the sentiment for the reviews can be classified as either positive or negative based on the words \"loved\" and \"disappointed\" in the review.\n",
      "<--- 7 --->\n",
      "<--- 8 --->\n",
      "<--- 8 --->\n",
      "<--- 9 --->\n",
      "<--- 9 --->\n",
      "\n",
      "---\n",
      "\n",
      "<--- 9 --->\n",
      "<--- 10 --->\n",
      "<--- 10 --->\n",
      "<--- 11 --->\n",
      "<--- 11 --->\n",
      "This approach is a great way to classify user sentiment. The reviews are classified either positively or negatively, which helps you understand customer feedback more accurately.\n",
      "---\n",
      "\n",
      "This approach is a great way to classify user sentiment. The reviews are classified either positively or negatively, which helps you understand customer feedback more accurately. \n",
      "\n",
      "For example, if a user says something like \"This product is amazing and exceeded my expectations,\" it's classified as positive. If the review says \"I was not impressed with this product,\" it's classified as negative. \n",
      "\n",
      "Understanding customer sentiment can help you build better products and improve customer experiences.\n",
      "<--- 11 --->\n",
      "<--- 12 --->\n",
      "<--- 12 --->\n",
      "<--- 13 --->\n",
      "<--- 13 --->\n",
      "The sentiment of the given reviews can be classified as either positive or negative, depending on the feedback. Reviews which contain words expressing satisfaction, happiness, and a high degree of quality are classified as positive, whereas those which contain words indicating dissatisfaction and a lower quality are classified as negative.\n",
      "---\n",
      "The sentiment of the given reviews can be classified as either positive or negative, depending on the feedback. Reviews which contain words expressing satisfaction, happiness, and a high degree of quality are classified as positive, whereas those which contain words indicating dissatisfaction and a lower quality are classified as negative.\n",
      "<--- 13 --->\n",
      "<--- 14 --->\n",
      "<--- 14 --->\n",
      "<--- 15 --->\n",
      "<--- 15 --->\n",
      "The sentiment of the review can be classified as either positive or negative depending on whether the review is praising or criticizing the product. The sentence \"Disappointing and frustrating experience\" can be classified as a negative sentiment.\n",
      "---\n",
      "The sentiment of the review can be classified as either positive or negative depending on whether the review is praising or criticizing the product. The sentence \"Disappointing and frustrating experience\" can be classified as a negative sentiment.\n",
      "<--- 15 --->\n",
      "<--- 16 --->\n",
      "<--- 16 --->\n",
      "<--- 17 --->\n",
      "<--- 17 --->\n",
      "Sentiment: Negative\n",
      "---\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "2. How likely are you to recommend this product to others? \n",
      "\n",
      "1. 6/10\n",
      "2. 8/10\n",
      "\n",
      "3. What changes would you suggest to improve the product?\n",
      "\n",
      "1. More positive reviews\n",
      "2. Better product selection\n",
      "3. Lower prices\n",
      "User \n",
      "<--- 17 --->\n",
      "<--- 18 --->\n",
      "<--- 18 --->\n",
      "<--- 19 --->\n",
      "<--- 19 --->\n",
      "The sentiment of the reviews can be classified as positive for the first two, and negative for the last two.\n",
      "---\n",
      "\n",
      "The sentiment of the reviews can be classified as positive for the first two, and negative for the last two.\n",
      "User \n",
      "<--- 19 --->\n",
      "<--- 20 --->\n",
      "<--- 20 --->\n",
      "<--- 21 --->\n",
      "<--- 21 --->\n",
      "Based on the reviews, the sentiment of each review can be classified as either positive or negative. The review that had a positive sentiment can be classified as \"Positive\", while the review with a negative sentiment can be classified as \"Negative\". The review with no explanation can be classified as \"No Explanation\" and the review that failed to deliver any of its promises can be classified as \"Disappointment\".\n",
      "---\n",
      "\n",
      "Based on the reviews, the sentiment of each review can be classified as either positive or negative. The review that had a positive sentiment can be classified as \"Positive\", while the review with a negative sentiment can be classified as \"Negative\". The review with no explanation can be classified as \"No Explanation\" and the review that failed to deliver any of its promises can be classified as \"Disappointment\".\n",
      "<--- 21 --->\n",
      "<--- 22 --->\n",
      "<--- 22 --->\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m correct_llama \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (review, sentiment) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m----> 5\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfalcon:7b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     sentiment_ext \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      7\u001b[0m     correct_llama \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sentiment_ext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m sentiment\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/ollama/_client.py:242\u001b[0m, in \u001b[0;36mClient.generate\u001b[0;34m(self, model, prompt, suffix, system, template, context, stream, raw, format, images, options, keep_alive)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    218\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    231\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[1;32m    232\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/ollama/_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/ollama/_client.py:118\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "correct_llama = 0\n",
    "\n",
    "for i, (review, sentiment) in enumerate(data):\n",
    "    gen = ollama.generate(model='falcon:7b', prompt=prompt.format(review=review))\n",
    "    sentiment_ext = gen.response.split(f'\"{review}\"')[-1].strip().split(\"\\n\")[0].strip()\n",
    "    correct_llama += sentiment_ext.strip() == sentiment.strip()\n",
    "    if not sentiment == 'Positive':\n",
    "        print(f'<--- {i} --->')\n",
    "        print(review + f\" [Correct: {sentiment}]\")\n",
    "        print('---')\n",
    "        print(gen.response)\n",
    "        print(f'<--- {i} --->')\n",
    "    \n",
    "print(f'{correct_llama}/{n_completions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab2969-bef0-476f-8cef-43944efd254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9a1baa-cd10-48b3-9e1c-9d9e06f5a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cfa74-9c81-452d-84d2-b108da22e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is the first document. Test test test\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db14d0-86a7-4afc-8b37-9d25c3139487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling Strategy\n",
    "attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "sum_embeddings = torch.sum(token_embeddings * attention_mask, 1)\n",
    "sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9)\n",
    "sentence_embedding = sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48252a16-72ae-4c82-ba33-a0f40a63d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Strategy\n",
    "max_embeddings, _ = torch.max(token_embeddings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92fca34c-d895-46a2-b8f7-b8c02001b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c10dc2-baa9-41d0-9c0f-21612415606b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
