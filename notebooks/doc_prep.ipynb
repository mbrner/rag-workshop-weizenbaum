{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich import print\n",
    "from rich.panel import Panel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MarkItdown\n",
    "[MarkItDown](https://github.com/microsoft/markitdown) is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc).\n",
    "It supports:\n",
    "\n",
    "- PDF\n",
    "- PowerPoint\n",
    "- Word\n",
    "- Excel\n",
    "- Images (EXIF metadata and OCR)\n",
    "- Audio (EXIF metadata and speech transcription)\n",
    "- HTML\n",
    "- Text-based formats (CSV, JSON, XML)\n",
    "- ZIP files (iterates over contents)\n",
    "- Youtube URLs\n",
    "- ... and more!\n",
    "\n",
    "To install MarkItDown, use pip: `pip install markitdown[all]`. Alternatively, you can install it from the source:\n",
    "\n",
    "```bash\n",
    "pip install markitdown[all]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "md = MarkItDown(llm_client=client, llm_model=\"gpt-4o-mini\")\n",
    "result = md.convert(\"papers/2501.07391v1.pdf\")\n",
    "\n",
    "print(Panel.fit(result.text_content, title=\"MarkItDown Output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docling\n",
    "\n",
    "\n",
    "[Docling](https://github.com/DS4SD/docling) simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem.\n",
    "\n",
    "## Features\n",
    "\n",
    "* 🗂️ Parsing of [multiple document formats][supported_formats] incl. PDF, DOCX, XLSX, HTML, images, and more\n",
    "* 📑 Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more\n",
    "* 🧬 Unified, expressive [DoclingDocument][docling_document] representation format\n",
    "* ↪️ Various [export formats][supported_formats] and options, including Markdown, HTML, and lossless JSON\n",
    "* 🔒 Local execution capabilities for sensitive data and air-gapped environments\n",
    "* 🤖 Plug-and-play [integrations][integrations] incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI\n",
    "* 🔍 Extensive OCR support for scanned PDFs and images\n",
    "* 💻 Simple and convenient CLI\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "input_doc_path = Path(\"papers/2501.07391v1.pdf\")\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    enable_remote_services=False\n",
    ")\n",
    "pipeline_options.do_picture_description = False\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "result = doc_converter.convert(input_doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Panel.fit(result.document.export_to_markdown(), title=\"Docling Output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def recursive_text_splitter(\n",
    "    text: str,\n",
    "    chunk_size: int = 1000,\n",
    "    min_chunk_size: int = 200,\n",
    "    chunk_overlap: int = 200,\n",
    "    separators: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text recursively using a list of separators, ensuring minimum chunk size.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to split\n",
    "        chunk_size: Maximum size of each chunk\n",
    "        min_chunk_size: Minimum size of each chunk\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        separators: List of separators to use in order of preference\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Default separators if none provided\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    \n",
    "    # Ensure min_chunk_size is not larger than chunk_size\n",
    "    min_chunk_size = min(min_chunk_size, chunk_size)\n",
    "    \n",
    "    def merge_chunks(splits: List[str], separator: str) -> List[str]:\n",
    "        \"\"\"Merge splits into chunks with overlap, ensuring minimum size.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for split in splits:\n",
    "            split_length = len(split) + (len(separator) if current_chunk else 0)\n",
    "            \n",
    "            # If adding this split would exceed chunk size, finalize current chunk\n",
    "            if current_length + split_length > chunk_size:\n",
    "                # Save current chunk if it meets minimum size\n",
    "                if current_chunk:\n",
    "                    chunk_text = separator.join(current_chunk)\n",
    "                    if len(chunk_text) >= min_chunk_size or not chunks:\n",
    "                        chunks.append(chunk_text)\n",
    "                    \n",
    "                    # Create overlap for next chunk\n",
    "                    overlap_chunks = []\n",
    "                    overlap_length = 0\n",
    "                    \n",
    "                    for item in reversed(current_chunk):\n",
    "                        sep_len = len(separator) if overlap_chunks else 0\n",
    "                        if len(item) + sep_len + overlap_length > chunk_overlap:\n",
    "                            break\n",
    "                        overlap_chunks.insert(0, item)\n",
    "                        overlap_length += len(item) + sep_len\n",
    "                    \n",
    "                    current_chunk = overlap_chunks\n",
    "                    current_length = overlap_length\n",
    "                \n",
    "                # Handle splits larger than chunk_size\n",
    "                if split_length > chunk_size:\n",
    "                    for i in range(0, len(split), chunk_size - chunk_overlap):\n",
    "                        chunk = split[i:min(i + chunk_size, len(split))]\n",
    "                        if len(chunk) >= min_chunk_size or not chunks:\n",
    "                            chunks.append(chunk)\n",
    "                    \n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "                    continue\n",
    "            \n",
    "            # Add to current chunk\n",
    "            current_chunk.append(split)\n",
    "            current_length += split_length\n",
    "        \n",
    "        # Handle the final chunk\n",
    "        if current_chunk:\n",
    "            final_text = separator.join(current_chunk)\n",
    "            if len(final_text) >= min_chunk_size or not chunks:\n",
    "                chunks.append(final_text)\n",
    "            elif chunks and len(chunks[-1]) + len(separator) + len(final_text) <= chunk_size:\n",
    "                # Merge with previous chunk if too small and fits\n",
    "                chunks[-1] = chunks[-1] + separator + final_text\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def split_text(text: str, level: int = 0) -> List[str]:\n",
    "        \"\"\"Split text using separators at current level.\"\"\"\n",
    "        # Base cases\n",
    "        if len(text) <= chunk_size:\n",
    "            return [text] if len(text) >= min_chunk_size or not text else []\n",
    "        \n",
    "        # If at the character level, chunk by size\n",
    "        if level >= len(separators) - 1:\n",
    "            chunks = []\n",
    "            for i in range(0, len(text), max(1, chunk_size - chunk_overlap)):\n",
    "                chunk = text[i:i + chunk_size]\n",
    "                if len(chunk) >= min_chunk_size or not chunks:\n",
    "                    chunks.append(chunk)\n",
    "            return chunks\n",
    "        \n",
    "        # Try to split with current separator\n",
    "        separator = separators[level]\n",
    "        splits = [char for char in text] if separator == \"\" else text.split(separator)\n",
    "        \n",
    "        # If splitting doesn't work, try next separator\n",
    "        if len(splits) <= 1:\n",
    "            return split_text(text, level + 1)\n",
    "        \n",
    "        # Process each split\n",
    "        results = []\n",
    "        for split in splits:\n",
    "            if len(split) <= chunk_size:\n",
    "                results.append(split)\n",
    "            else:\n",
    "                results.extend(split_text(split, level + 1))\n",
    "        \n",
    "        # Merge the results\n",
    "        return merge_chunks(results, separator)\n",
    "    \n",
    "    return split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = recursive_text_splitter(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def calculate_distances(sentences: List[str], buffer_size: int = 3) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculates semantic distances between adjacent sentences with context.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentence strings\n",
    "        client: OpenAI client\n",
    "        buffer_size: Number of sentences to include as context before and after\n",
    "        \n",
    "    Returns:\n",
    "        distances: List of semantic distances between adjacent sentences\n",
    "    \"\"\"\n",
    "    # Calculate embeddings directly in batches\n",
    "    BATCH_SIZE = 500\n",
    "    embedding_matrix = None\n",
    "    \n",
    "    # Process sentences in batches, combining with context on the fly\n",
    "    for batch_start in range(0, len(sentences), BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(sentences))\n",
    "        \n",
    "        # Create combined sentences for this batch\n",
    "        batch_combined = []\n",
    "        for i in range(batch_start, batch_end):\n",
    "            context_start = max(0, i - buffer_size)\n",
    "            context_end = min(len(sentences), i + buffer_size + 1)\n",
    "            combined = ' '.join(sentences[context_start:context_end])\n",
    "            batch_combined.append(combined)\n",
    "        \n",
    "        # Get embeddings for this batch using OpenAI API\n",
    "        response = client.embeddings.create(model='text-embedding-3-small', input=batch_combined)\n",
    "        batch_embeddings = np.array([item.embedding for item in response.data])\n",
    "        \n",
    "        if embedding_matrix is None:\n",
    "            embedding_matrix = batch_embeddings\n",
    "        else:\n",
    "            embedding_matrix = np.concatenate((embedding_matrix, batch_embeddings), axis=0)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "    embedding_matrix = embedding_matrix / norms\n",
    "    \n",
    "    # Calculate similarity matrix and extract distances between adjacent sentences\n",
    "    similarity_matrix = np.dot(embedding_matrix, embedding_matrix.T)\n",
    "    distances = [1 - similarity_matrix[i, i + 1] for i in range(len(sentences) - 1)]\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "def get_cut_indices(distances, target_cuts):\n",
    "    \"\"\"\n",
    "    Find cut indices based on semantic distances and target number of cuts.\n",
    "    \n",
    "    Args:\n",
    "        distances: List of semantic distances between adjacent sentences\n",
    "        target_cuts: Target number of cuts\n",
    "    \n",
    "    Returns:\n",
    "        List of cut indices\n",
    "    \"\"\"\n",
    "    # Binary search for optimal threshold\n",
    "    lower_limit, upper_limit = 0.0, 1.0\n",
    "    distances_np = np.array(distances)\n",
    "    \n",
    "    while upper_limit - lower_limit > 1e-6:\n",
    "        threshold = (upper_limit + lower_limit) / 2.0\n",
    "        cuts = np.sum(distances_np > threshold)\n",
    "        \n",
    "        if cuts > target_cuts:\n",
    "            lower_limit = threshold\n",
    "        else:\n",
    "            upper_limit = threshold\n",
    "    \n",
    "    # Find cut points based on threshold\n",
    "    cut_indices = [i for i, d in enumerate(distances) if d > threshold] + [-1]\n",
    "    \n",
    "    return cut_indices\n",
    "\n",
    "def semantic_text_splitter(text: str, \n",
    "                           avg_chunk_size: int = 1600, \n",
    "                           min_chunk_size: int = 800,\n",
    "                           max_chunk_size: int = 4000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks of approximately avg_chunk_size characters based on semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to be split\n",
    "        client: OpenAI client instance\n",
    "        avg_chunk_size: Target average size of chunks in characters\n",
    "        min_chunk_size: Minimum size for initial text splitting\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"    \n",
    "    # Split text into minimal sentence units\n",
    "    sentences = recursive_text_splitter(text, min_chunk_size, int(min_chunk_size*0.5), chunk_overlap=0)\n",
    "    # Calculate distances between sentences\n",
    "    distances = calculate_distances(sentences)\n",
    "    \n",
    "    # Determine number of cuts needed based on character count\n",
    "    total_length = sum(len(s) for s in sentences)\n",
    "    target_cuts = total_length // avg_chunk_size\n",
    "    \n",
    "    cut_indices = get_cut_indices(distances, target_cuts)\n",
    "    \n",
    "    # Create chunks based on cut points\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    sentence_pointer = 0\n",
    "    while sentence_pointer < len(sentences):\n",
    "        sentence = sentences[sentence_pointer]\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = ''\n",
    "            cut_indices = [n+sentence_pointer for n in get_cut_indices(distances[sentence_pointer:], target_cuts-len(chunks))]\n",
    "            continue\n",
    "        if len(sentence) < int(min_chunk_size*0.5):\n",
    "            print(sentence)\n",
    "        current_chunk += f'\\n{sentence}' if current_chunk else sentence\n",
    "        if sentence_pointer == cut_indices[0]:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = ''\n",
    "            cut_indices.pop(0)\n",
    "        sentence_pointer += 1\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = semantic_text_splitter(result.document.export_to_markdown(), 1600, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(Panel.fit(chunks[i], title=f\"Chunk {i+1}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an assistant specialized in splitting text into thematically consistent sections.\"\n",
    "\n",
    "USER_MSG = \"\"\"The text has been divided into chunks, each marked with <|start_chunk_X|> and <|end_chunk_X|> tags, where X is the chunk number.\n",
    "Your task is to identify the points where splits should occur, such that consecutive chunks of similar themes stay together. Try to avoid splitting in the middle of a topic/section/paragraph\n",
    "\n",
    "{chunked_input}\n",
    "\n",
    "Respond with a list of chunk IDs where you believe a split should be made. For example, if chunks 1 and 2 belong together but chunk 3 starts a new topic, you would suggest a split after chunk 2.\n",
    "THE CHUNKS MUST BE IN ASCENDING ORDER.\n",
    "Your response should be in the form: 'split_after: 3, 5'\n",
    "Respond only with the IDs of the chunks where you believe a split should occur.\n",
    "YOU MUST RESPOND WITH AT LEAST ONE SPLIT. THESE SPLITS MUST BE IN ASCENDING ORDER AND EQUAL OR LARGER THAN: {current_chunk}\n",
    "\"\"\"\n",
    "\n",
    "def llm_text_splitter(text,\n",
    "                      min_chunk_size: int = 800,\n",
    "                      n_chunks_per_prompt: int = 10,\n",
    "                      max_retries: int = 5):\n",
    "    chunks = recursive_text_splitter(text, min_chunk_size, int(min_chunk_size*0.5), chunk_overlap=0)\n",
    "    split_indices = []\n",
    "    current_chunk = 0\n",
    "    while True:\n",
    "        if current_chunk >= len(chunks) - 4:\n",
    "            break\n",
    "        chunked_input = []\n",
    "        for i in range(current_chunk, min(len(chunks), current_chunk+n_chunks_per_prompt)):\n",
    "            chunked_input.append(f\"<|start_chunk_{i+1}|>{chunks[i]}<|end_chunk_{i+1}|>\")\n",
    "        chunked_input = '\\n'.join(chunked_input)\n",
    "        original_prompt = USER_MSG.format(chunked_input=chunked_input, current_chunk=current_chunk)\n",
    "        prompt = original_prompt\n",
    "        final_answer = None\n",
    "        for _ in range(max_retries):\n",
    "            result_string = client.chat.completions.create(model='gpt-4o-mini', messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt}], max_tokens=200, temperature=0.2)\n",
    "            result_string = result_string.choices[0].message.content\n",
    "            split_after_line = [line for line in result_string.split('\\n') if 'split_after:' in line][0]\n",
    "            numbers = re.findall(r'\\d+', split_after_line)\n",
    "            numbers = list(map(int, numbers))\n",
    "            if not (numbers != sorted(numbers) or any(number < current_chunk for number in numbers)):\n",
    "                final_answer = numbers\n",
    "                break\n",
    "            else:\n",
    "                prompt = original_prompt + f\"\\nThe previous response of {numbers} was invalid. DO NOT REPEAT THIS ARRAY OF NUMBERS. Please try again.\" \n",
    "        if final_answer is None:\n",
    "            raise ValueError(\"Failed to retrieve valid split\")\n",
    "        split_indices.extend(final_answer)\n",
    "        current_chunk = numbers[-1]\n",
    "        if len(numbers) == 0:\n",
    "            break\n",
    "    chunks_to_split_after = [i - 1 for i in split_indices]\n",
    "    docs = []\n",
    "    current_chunk = ''\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk += chunk + ' '\n",
    "        if i in chunks_to_split_after:\n",
    "            docs.append(current_chunk.strip())\n",
    "            current_chunk = ''\n",
    "    if current_chunk:\n",
    "        docs.append(current_chunk.strip())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_llm = llm_text_splitter(result.document.export_to_markdown(), 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(Panel.fit(chunks_llm[i], title=f\"Chunk {i+1}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs for OCR + Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKING_PROMPT = \"\"\"\\\n",
    "OCR the following page into Markdown. Tables should be formatted as HTML. \n",
    "Do not sorround your output with triple backticks.\n",
    "\n",
    "Chunk the document into sections of roughly 250 - 1000 words. Our goal is \n",
    "to identify parts of the page with same semantic theme. These chunks will \n",
    "be embedded and used in a RAG pipeline. \n",
    "\n",
    "Surround the chunks with <chunk-page-X> </chunk-page-X> tags and X should be the page on which the the chunk starts. Keep paragarphs and sections, but ignore linebreaks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=os.environ[\"GCP_PROJECT\"], location=os.environ[\"GCP_LOCATION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I545048/personal/rag-book/.venv/lib/python3.11/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to find your project. Please provide a project ID by:\n- Passing a constructor argument\n- Using vertexai.init()\n- Setting project using 'gcloud config set project my-project'\n- Setting a GCP environment variable\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerativeModel, Part\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGenerativeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.0-flash-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# If your image is stored in Google Cloud Storage, you can use the from_uri class method to create a Part object.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpapers/2501.07391v1.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py:353\u001b[0m, in \u001b[0;36m_GenerativeModel.__init__\u001b[0;34m(self, model_name, generation_config, safety_settings, tools, tool_config, system_instruction, labels)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    324\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m     labels: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m ):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes GenerativeModel.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    Usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m        labels: labels that will be passed to billing for cost tracking.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform_initializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject\u001b[49m\n\u001b[1;32m    354\u001b[0m     location \u001b[38;5;241m=\u001b[39m aiplatform_initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mlocation\n\u001b[1;32m    355\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m _reconcile_model_name(model_name, project, location)\n",
      "File \u001b[0;32m~/personal/rag-book/.venv/lib/python3.11/site-packages/google/cloud/aiplatform/initializer.py:362\u001b[0m, in \u001b[0;36m_Config.project\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GoogleAuthError(project_not_found_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m project_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(project_not_found_exception_str)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m project_id\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to find your project. Please provide a project ID by:\n- Passing a constructor argument\n- Using vertexai.init()\n- Setting project using 'gcloud config set project my-project'\n- Setting a GCP environment variable\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project"
     ]
    }
   ],
   "source": [
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "\n",
    "model = GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "\n",
    "# If your image is stored in Google Cloud Storage, you can use the from_uri class method to create a Part object.\n",
    "with open(\"papers/2501.07391v1.pdf\", \"rb\") as f:\n",
    "    pdf_data = f.read()\n",
    "response = GenerativeModel(model_name='gemini-2.0-flash-001').generate_content(\n",
    "  contents=[\n",
    "    CHUNKING_PROMPT,\n",
    "    Part.from_data(\n",
    "        data=pdf_data,\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Panel.fit(response.text, title=\"Gemini Output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMS for OCR + Chunking + Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GCP_PROJECT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(\n\u001b[0;32m----> 5\u001b[0m   vertexai\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, project\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGCP_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, location\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGCP_LOCATION\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerativeModel, Part\n\u001b[1;32m      9\u001b[0m RETRIEVAL_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mOCR the following page into Markdown. Tables should be formatted as HTML. Keep paragarphs and sections, but ignore linebreaks.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mDo not sorround your output with triple backticks.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m...\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'GCP_PROJECT'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(\n",
    "  vertexai=True, project=os.environ[\"GCP_PROJECT\"], location=os.environ[\"GCP_LOCATION\"]\n",
    ")\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "RETRIEVAL_PROMPT = \"\"\"\\\n",
    "OCR the following page into Markdown. Tables should be formatted as HTML. Keep paragarphs and sections, but ignore linebreaks.\n",
    "Do not sorround your output with triple backticks.\n",
    "Return the chunks from the document that contain most relevant information on the following questions/topen: '{topic}'.\n",
    "Don't return more than {num_chunks} chunks. Surround the relevant pages with <chunk-page-X>exact text of the chunk</chunk-page-X> tags and X should be number of the page from with the chunk is returned.\n",
    "If less than {num_chunks} chunks are relevant return only the relevant chunks. For every chunk return at least 100 words to give enough context.\n",
    "So for example, if pages 10,146, and 4 contain text chunks of relevance. The output should look like this.\n",
    "If the document is shorter than {num_pages} pages, return all pages of the document and use 1 chunk per page.\n",
    "<chunk-page-10>\n",
    " The prompt shapes how the model interprets its task and utilizes retrieved information (Sun et al., 2024).    \n",
    "...                                                                          \n",
    "Q4. How does the size of the knowledge base impact the overall performance? We examine the effect of different\n",
    "knowledge base sizes in terms of the number of documents.                                                                                                \n",
    "</chunk-page-10>\n",
    "<chunk-page-145>\n",
    "Small prompt changes may influence alignment, affecting response quality. We not only examine these small    \n",
    "variations but also test counterfactual prompts, to explore the model's behavior under opposite guidance and \n",
    "...     \n",
    "how different prompt crafting strategies can optimize performance.                                           \n",
    "Q3. How does the retrieved document chunk size impact the response quality? Chunk size affects the balance \n",
    "</chunk-page-145>\n",
    "<chunk-page-4>\n",
    "...\n",
    "</chunk-page-4>\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# If your image is stored in Google Cloud Storage, you can use the from_uri class method to create a Part object.\n",
    "with open(\"papers/eu-ai-act.pdf\", \"rb\") as f: # eu-ai-act.pdf\n",
    "    pdf_data = f.read()\n",
    "response = GenerativeModel(model_name='gemini-2.0-flash-001').generate_content(\n",
    "  contents=[\n",
    "    RETRIEVAL_PROMPT.format(topic=\"How high are the fines for companies not compliant with the EU AI ACT?\", num_pages=5, num_chunks=5),\n",
    "    Part.from_data(\n",
    "        data=pdf_data,\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Gemini Output ─────────────────────────────────────────╮\n",
       "│ &lt;chunk-page-1&gt;                                                                                 │\n",
       "│ Software Provider Contract for Cloud Service                                                   │\n",
       "│ This Agreement (\"Agreement\") is made and entered into as of 01.08.2024, by and between:        │\n",
       "│ Provider:                                                                                      │\n",
       "│ SAP SE                                                                                         │\n",
       "│ Dietmar-Hopp Allee 16                                                                          │\n",
       "│ 69190 Walldorf                                                                                 │\n",
       "│ Customer:                                                                                      │\n",
       "│ Friedrich Bismarck                                                                             │\n",
       "│ Dietmar Hopp Allee 16                                                                          │\n",
       "│ 69190 Walldorf                                                                                 │\n",
       "│ Whereas, the Provider offers a Cloud Service, and the Customer wishes to subscribe to the      │\n",
       "│ Cloud Service under the terms set forth in this Agreement.                                     │\n",
       "│ 1. Product Description                                                                         │\n",
       "│ The Provider agrees to provide the Customer with access to the Cloud Service, which            │\n",
       "│ includes enterprise of features, functionalities, and benefits of the Cloud Service. The Cloud │\n",
       "│ Service is a application to speed up billing processes, available on a subscription basis,     │\n",
       "│ designed to meet the business needs of the Customer.                                           │\n",
       "│ 2. Subscription Fees                                                                           │\n",
       "│ • Base Subscription Fee: The Customer agrees to pay a monthly subscription fee of              │\n",
       "│ €200 for the Cloud Service.                                                                    │\n",
       "│ • Annual Price Increase: At the beginning of each new calendar year, the monthly               │\n",
       "│ subscription fee will increase by 2%.                                                          │\n",
       "│ • Subscription fees are due at the beginning of each month, and the Customer agrees            │\n",
       "│ to pay on time via the agreed payment method.                                                  │\n",
       "│ • The subscription fee is calculated on a monthly basis, and no refunds will be provided       │\n",
       "│ for partial months.                                                                            │\n",
       "│ 3. Term and Renewal                                                                            │\n",
       "│ • Initial Term: This Agreement will commence on [Start Date] and will continue for a           │\n",
       "│ period of one year.                                                                            │\n",
       "│ • Automatic Renewal: After the initial term, this Agreement will automatically renew           │\n",
       "│ for successive one-year periods unless either party provides written notice of non-            │\n",
       "│ renewal at least 30 days before the end of the current term.                                   │\n",
       "│ • Price Adjustments: The monthly subscription fee will increase by 2% each year at the         │\n",
       "│ beginning of each new calendar year, as outlined in Section 2.                                 │\n",
       "│ &lt;/chunk-page-1&gt;                                                                                │\n",
       "│                                                                                                │\n",
       "╰────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Gemini Output ─────────────────────────────────────────╮\n",
       "│ <chunk-page-1>                                                                                 │\n",
       "│ Software Provider Contract for Cloud Service                                                   │\n",
       "│ This Agreement (\"Agreement\") is made and entered into as of 01.08.2024, by and between:        │\n",
       "│ Provider:                                                                                      │\n",
       "│ SAP SE                                                                                         │\n",
       "│ Dietmar-Hopp Allee 16                                                                          │\n",
       "│ 69190 Walldorf                                                                                 │\n",
       "│ Customer:                                                                                      │\n",
       "│ Friedrich Bismarck                                                                             │\n",
       "│ Dietmar Hopp Allee 16                                                                          │\n",
       "│ 69190 Walldorf                                                                                 │\n",
       "│ Whereas, the Provider offers a Cloud Service, and the Customer wishes to subscribe to the      │\n",
       "│ Cloud Service under the terms set forth in this Agreement.                                     │\n",
       "│ 1. Product Description                                                                         │\n",
       "│ The Provider agrees to provide the Customer with access to the Cloud Service, which            │\n",
       "│ includes enterprise of features, functionalities, and benefits of the Cloud Service. The Cloud │\n",
       "│ Service is a application to speed up billing processes, available on a subscription basis,     │\n",
       "│ designed to meet the business needs of the Customer.                                           │\n",
       "│ 2. Subscription Fees                                                                           │\n",
       "│ • Base Subscription Fee: The Customer agrees to pay a monthly subscription fee of              │\n",
       "│ €200 for the Cloud Service.                                                                    │\n",
       "│ • Annual Price Increase: At the beginning of each new calendar year, the monthly               │\n",
       "│ subscription fee will increase by 2%.                                                          │\n",
       "│ • Subscription fees are due at the beginning of each month, and the Customer agrees            │\n",
       "│ to pay on time via the agreed payment method.                                                  │\n",
       "│ • The subscription fee is calculated on a monthly basis, and no refunds will be provided       │\n",
       "│ for partial months.                                                                            │\n",
       "│ 3. Term and Renewal                                                                            │\n",
       "│ • Initial Term: This Agreement will commence on [Start Date] and will continue for a           │\n",
       "│ period of one year.                                                                            │\n",
       "│ • Automatic Renewal: After the initial term, this Agreement will automatically renew           │\n",
       "│ for successive one-year periods unless either party provides written notice of non-            │\n",
       "│ renewal at least 30 days before the end of the current term.                                   │\n",
       "│ • Price Adjustments: The monthly subscription fee will increase by 2% each year at the         │\n",
       "│ beginning of each new calendar year, as outlined in Section 2.                                 │\n",
       "│ </chunk-page-1>                                                                                │\n",
       "│                                                                                                │\n",
       "╰────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Panel.fit(response.text, title=\"Gemini Output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt_token_count: 1197\n",
       "candidates_token_count: 417\n",
       "total_token_count: 1614"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002865"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.prompt_token_count / 1_000_000 * 0.1 + response.usage_metadata.candidates_token_count / 1_000_000 * 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
